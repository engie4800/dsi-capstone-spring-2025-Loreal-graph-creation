{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (1.65.4)\n",
      "Requirement already satisfied: graphdatascience in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (1.12)\n",
      "Requirement already satisfied: retry==0.9.2 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (0.9.2)\n",
      "Requirement already satisfied: langchain>=0.0.216 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (0.3.19)\n",
      "Requirement already satisfied: streamlit==1.23.1 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (1.23.1)\n",
      "Requirement already satisfied: streamlit-chat==0.0.2.2 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (0.0.2.2)\n",
      "Requirement already satisfied: streamlit-chat-media==0.0.4 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (0.0.4)\n",
      "Requirement already satisfied: numpy in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from -r requirements.txt (line 8)) (1.26.4)\n",
      "Requirement already satisfied: pandas in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from -r requirements.txt (line 9)) (2.2.3)\n",
      "Requirement already satisfied: plotly==5.15.0 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from -r requirements.txt (line 10)) (5.15.0)\n",
      "Requirement already satisfied: altair<5 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from -r requirements.txt (line 11)) (4.2.2)\n",
      "Requirement already satisfied: python-dotenv==1.0.0 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from -r requirements.txt (line 12)) (1.0.0)\n",
      "Requirement already satisfied: decorator>=3.4.2 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from retry==0.9.2->-r requirements.txt (line 3)) (5.1.1)\n",
      "Requirement already satisfied: py<2.0.0,>=1.4.26 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from retry==0.9.2->-r requirements.txt (line 3)) (1.11.0)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from streamlit==1.23.1->-r requirements.txt (line 5)) (1.9.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from streamlit==1.23.1->-r requirements.txt (line 5)) (5.5.1)\n",
      "Requirement already satisfied: click<9,>=7.0 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from streamlit==1.23.1->-r requirements.txt (line 5)) (8.1.8)\n",
      "Requirement already satisfied: importlib-metadata<7,>=1.4 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from streamlit==1.23.1->-r requirements.txt (line 5)) (6.11.0)\n",
      "Requirement already satisfied: packaging<24,>=14.1 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from streamlit==1.23.1->-r requirements.txt (line 5)) (23.2)\n",
      "Requirement already satisfied: pillow<10,>=6.2.0 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from streamlit==1.23.1->-r requirements.txt (line 5)) (9.5.0)\n",
      "Requirement already satisfied: protobuf<5,>=3.20 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from streamlit==1.23.1->-r requirements.txt (line 5)) (4.25.6)\n",
      "Requirement already satisfied: pyarrow>=4.0 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from streamlit==1.23.1->-r requirements.txt (line 5)) (16.1.0)\n",
      "Requirement already satisfied: pympler<2,>=0.9 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from streamlit==1.23.1->-r requirements.txt (line 5)) (1.1)\n",
      "Requirement already satisfied: python-dateutil<3,>=2 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from streamlit==1.23.1->-r requirements.txt (line 5)) (2.9.0.post0)\n",
      "Requirement already satisfied: requests<3,>=2.4 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from streamlit==1.23.1->-r requirements.txt (line 5)) (2.32.3)\n",
      "Requirement already satisfied: rich<14,>=10.11.0 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from streamlit==1.23.1->-r requirements.txt (line 5)) (13.9.4)\n",
      "Requirement already satisfied: tenacity<9,>=8.0.0 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from streamlit==1.23.1->-r requirements.txt (line 5)) (8.5.0)\n",
      "Requirement already satisfied: toml<2 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from streamlit==1.23.1->-r requirements.txt (line 5)) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.0.1 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from streamlit==1.23.1->-r requirements.txt (line 5)) (4.12.2)\n",
      "Requirement already satisfied: tzlocal<5,>=1.1 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from streamlit==1.23.1->-r requirements.txt (line 5)) (4.3.1)\n",
      "Requirement already satisfied: validators<1,>=0.2 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from streamlit==1.23.1->-r requirements.txt (line 5)) (0.34.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from streamlit==1.23.1->-r requirements.txt (line 5)) (3.1.44)\n",
      "Requirement already satisfied: pydeck<1,>=0.1.dev5 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from streamlit==1.23.1->-r requirements.txt (line 5)) (0.9.1)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from streamlit==1.23.1->-r requirements.txt (line 5)) (6.4.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from openai->-r requirements.txt (line 1)) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from openai->-r requirements.txt (line 1)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from openai->-r requirements.txt (line 1)) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from openai->-r requirements.txt (line 1)) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from openai->-r requirements.txt (line 1)) (2.10.6)\n",
      "Requirement already satisfied: sniffio in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from openai->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from openai->-r requirements.txt (line 1)) (4.67.1)\n",
      "Requirement already satisfied: multimethod<2.0,>=1.0 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from graphdatascience->-r requirements.txt (line 2)) (1.12)\n",
      "Requirement already satisfied: neo4j<6.0,>=4.4.12 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from graphdatascience->-r requirements.txt (line 2)) (5.28.1)\n",
      "Requirement already satisfied: textdistance<5.0,>=4.0 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from graphdatascience->-r requirements.txt (line 2)) (4.6.3)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.35 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from langchain>=0.0.216->-r requirements.txt (line 4)) (0.3.36)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from langchain>=0.0.216->-r requirements.txt (line 4)) (0.3.6)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from langchain>=0.0.216->-r requirements.txt (line 4)) (0.3.8)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from langchain>=0.0.216->-r requirements.txt (line 4)) (1.4.54)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from langchain>=0.0.216->-r requirements.txt (line 4)) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from langchain>=0.0.216->-r requirements.txt (line 4)) (3.11.12)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from pandas->-r requirements.txt (line 9)) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from pandas->-r requirements.txt (line 9)) (2025.1)\n",
      "Requirement already satisfied: entrypoints in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from altair<5->-r requirements.txt (line 11)) (0.4)\n",
      "Requirement already satisfied: jinja2 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from altair<5->-r requirements.txt (line 11)) (3.1.5)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from altair<5->-r requirements.txt (line 11)) (4.23.0)\n",
      "Requirement already satisfied: toolz in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from altair<5->-r requirements.txt (line 11)) (1.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.216->-r requirements.txt (line 4)) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.216->-r requirements.txt (line 4)) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.216->-r requirements.txt (line 4)) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.216->-r requirements.txt (line 4)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.216->-r requirements.txt (line 4)) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.216->-r requirements.txt (line 4)) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.216->-r requirements.txt (line 4)) (1.18.3)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from gitpython!=3.1.19,<4,>=3->streamlit==1.23.1->-r requirements.txt (line 5)) (4.0.12)\n",
      "Requirement already satisfied: certifi in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 1)) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 1)) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 1)) (0.14.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from importlib-metadata<7,>=1.4->streamlit==1.23.1->-r requirements.txt (line 5)) (3.21.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from jsonschema>=3.0->altair<5->-r requirements.txt (line 11)) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from jsonschema>=3.0->altair<5->-r requirements.txt (line 11)) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from jsonschema>=3.0->altair<5->-r requirements.txt (line 11)) (0.22.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.35->langchain>=0.0.216->-r requirements.txt (line 4)) (1.33)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain>=0.0.216->-r requirements.txt (line 4)) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain>=0.0.216->-r requirements.txt (line 4)) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain>=0.0.216->-r requirements.txt (line 4)) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 1)) (2.27.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from jinja2->altair<5->-r requirements.txt (line 11)) (3.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from python-dateutil<3,>=2->streamlit==1.23.1->-r requirements.txt (line 5)) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from requests<3,>=2.4->streamlit==1.23.1->-r requirements.txt (line 5)) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from requests<3,>=2.4->streamlit==1.23.1->-r requirements.txt (line 5)) (2.3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from rich<14,>=10.11.0->streamlit==1.23.1->-r requirements.txt (line 5)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from rich<14,>=10.11.0->streamlit==1.23.1->-r requirements.txt (line 5)) (2.19.1)\n",
      "Requirement already satisfied: pytz-deprecation-shim in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from tzlocal<5,>=1.1->streamlit==1.23.1->-r requirements.txt (line 5)) (0.1.0.post0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3->streamlit==1.23.1->-r requirements.txt (line 5)) (5.0.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.35->langchain>=0.0.216->-r requirements.txt (line 4)) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/brandonkeung/.pyenv/versions/3.11.9/envs/Capstone311/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.11.0->streamlit==1.23.1->-r requirements.txt (line 5)) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from string import Template\n",
    "import json\n",
    "from neo4j import GraphDatabase\n",
    "import glob\n",
    "from timeit import default_timer as timer\n",
    "from dotenv import load_dotenv\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv('.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API configuration\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# openai.api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "openai_deployment = \"gpt-4\"\n",
    "# print(openai.api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neo4j configuration & constraints\n",
    "neo4j_url = os.getenv(\"NEO4J_CONNECTION_URL\")\n",
    "neo4j_user = os.getenv(\"NEO4J_USER\")\n",
    "neo4j_password = os.getenv(\"NEO4J_PASSWORD\")\n",
    "gds = GraphDatabase.driver(neo4j_url, auth=(neo4j_user, neo4j_password))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to call the OpenAI API\n",
    "def process_gpt(file_prompt, system_msg):\n",
    "    client = openai.OpenAI()  # Create a client instance\n",
    "    completion = client.chat.completions.create(  # Use the new API structure\n",
    "        model=openai_deployment,  # Update this to the correct model name, e.g., \"gpt-4\"\n",
    "        max_tokens=1000,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_msg},\n",
    "            {\"role\": \"user\", \"content\": file_prompt},\n",
    "        ],\n",
    "    )\n",
    "    nlp_results = completion.choices[0].message.content\n",
    "    sleep(8)\n",
    "    return nlp_results\n",
    "\n",
    "\n",
    "# def extract_entities_relationships(folder, prompt_template):\n",
    "#     start = timer()\n",
    "#     files = glob.glob(f\"./data/{folder}/*\")\n",
    "#     system_msg = \"You are a beauty and personal care expert who understands product relationships and routines.\"\n",
    "#     print(f\"Running pipeline for {len(files)} files in {folder} folder\")\n",
    "    \n",
    "#     # Maintain context across products\n",
    "#     product_context = {\n",
    "#         'brands': set(),\n",
    "#         'price_tiers': {\n",
    "#             'budget': [],\n",
    "#             'mid_range': [],\n",
    "#             'premium': []\n",
    "#         },\n",
    "#         'rating_tiers': {\n",
    "#             'excellent': [],\n",
    "#             'good': [],\n",
    "#             'average': [],\n",
    "#             'poor': []\n",
    "#         },\n",
    "#         'categories': {},\n",
    "#         'concerns': {},\n",
    "#         'ingredients': set()\n",
    "#     }\n",
    "    \n",
    "#     all_entities = {}\n",
    "#     all_relationships = set()\n",
    "    \n",
    "#     try:\n",
    "#         # First pass: collect product information\n",
    "#         for file in files:\n",
    "#             try:\n",
    "#                 with open(file, \"r\", encoding='utf-8') as f:\n",
    "#                     text = f.read().rstrip()\n",
    "#                     prompt = Template(prompt_template).substitute(ctext=text)\n",
    "#                     result = process_gpt(prompt, system_msg=system_msg)\n",
    "#                     parsed_result = json.loads(result)\n",
    "                    \n",
    "#                     # Update product context\n",
    "#                     for entity in parsed_result[\"entities\"]:\n",
    "#                         if entity[\"label\"] == \"Product\":\n",
    "#                             # Categorize by price tier\n",
    "#                             price = entity.get(\"price\", 0)\n",
    "#                             if price < 15:\n",
    "#                                 product_context['price_tiers']['budget'].append(entity[\"id\"])\n",
    "#                             elif price < 30:\n",
    "#                                 product_context['price_tiers']['mid_range'].append(entity[\"id\"])\n",
    "#                             else:\n",
    "#                                 product_context['price_tiers']['premium'].append(entity[\"id\"])\n",
    "                                \n",
    "#                             # Categorize by rating tier\n",
    "#                             rating = entity.get(\"rating\", 0)\n",
    "#                             if rating >= 4.5:\n",
    "#                                 product_context['rating_tiers']['excellent'].append(entity[\"id\"])\n",
    "#                             elif rating >= 4.0:\n",
    "#                                 product_context['rating_tiers']['good'].append(entity[\"id\"])\n",
    "#                             elif rating >= 3.0:\n",
    "#                                 product_context['rating_tiers']['average'].append(entity[\"id\"])\n",
    "#                             else:\n",
    "#                                 product_context['rating_tiers']['poor'].append(entity[\"id\"])\n",
    "                                \n",
    "#                             # Track categories and brands\n",
    "#                             if \"category\" in entity:\n",
    "#                                 product_context['categories'].setdefault(entity[\"category\"], []).append(entity[\"id\"])\n",
    "#                             if \"brand\" in entity:\n",
    "#                                 product_context['brands'].add(entity[\"brand\"])\n",
    "                            \n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error in first pass processing {file}: {e}\")\n",
    "#                 continue\n",
    "\n",
    "#         # Second pass: create relationships using context\n",
    "#         for file in files:\n",
    "#             try:\n",
    "#                 with open(file, \"r\", encoding='utf-8') as f:\n",
    "#                     text = f.read().rstrip()\n",
    "#                     prompt = Template(prompt_template).substitute(ctext=text, context=str(product_context))\n",
    "#                     result = process_gpt(prompt, system_msg=system_msg)\n",
    "#                     parsed_result = json.loads(result)\n",
    "                    \n",
    "#                     # Add entities and relationships\n",
    "#                     for entity in parsed_result[\"entities\"]:\n",
    "#                         entity_id = entity[\"id\"]\n",
    "#                         if entity_id in all_entities:\n",
    "#                             all_entities[entity_id].update(entity)\n",
    "#                         else:\n",
    "#                             all_entities[entity_id] = entity\n",
    "                    \n",
    "#                     all_relationships.update(parsed_result[\"relationships\"])\n",
    "                    \n",
    "#                     # Add contextual relationships\n",
    "#                     current_product_id = next(e[\"id\"] for e in parsed_result[\"entities\"] if e[\"label\"] == \"Product\")\n",
    "                    \n",
    "#                     # Add price tier relationships\n",
    "#                     for product_id in product_context['price_tiers'][get_price_tier(current_product_id, all_entities)]:\n",
    "#                         if product_id != current_product_id:\n",
    "#                             all_relationships.add(f\"{current_product_id}|SIMILAR_PRICE_TIER_AS|{product_id}\")\n",
    "                    \n",
    "#                     # Add rating tier relationships\n",
    "#                     for product_id in product_context['rating_tiers'][get_rating_tier(current_product_id, all_entities)]:\n",
    "#                         if product_id != current_product_id:\n",
    "#                             all_relationships.add(f\"{current_product_id}|SIMILAR_RATING_AS|{product_id}\")\n",
    "                    \n",
    "#                     # Add category relationships\n",
    "#                     current_category = all_entities[current_product_id].get(\"category\")\n",
    "#                     if current_category and current_category in product_context['categories']:\n",
    "#                         for product_id in product_context['categories'][current_category]:\n",
    "#                             if product_id != current_product_id:\n",
    "#                                 all_relationships.add(f\"{current_product_id}|SAME_CATEGORY_AS|{product_id}\")\n",
    "                    \n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error in second pass processing {file}: {e}\")\n",
    "#                 continue\n",
    "                \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in main processing: {e}\")\n",
    "#         return []\n",
    "        \n",
    "#     final_result = {\n",
    "#         \"entities\": list(all_entities.values()),\n",
    "#         \"relationships\": list(all_relationships)\n",
    "#     }\n",
    "    \n",
    "#     end = timer()\n",
    "#     print(f\"Pipeline completed in {end-start} seconds\")\n",
    "#     return [final_result]\n",
    "\n",
    "# def get_price_tier(product_id, entities):\n",
    "#     price = entities[product_id].get(\"price\", 0)\n",
    "#     if price < 15:\n",
    "#         return \"budget\"\n",
    "#     elif price < 30:\n",
    "#         return \"mid_range\"\n",
    "#     return \"premium\"\n",
    "\n",
    "# def get_rating_tier(product_id, entities):\n",
    "#     rating = entities[product_id].get(\"rating\", 0)\n",
    "#     if rating >= 4.5:\n",
    "#         return \"excellent\"\n",
    "#     elif rating >= 4.0:\n",
    "#         return \"good\"\n",
    "#     elif rating >= 3.0:\n",
    "#         return \"average\"\n",
    "#     return \"poor\"\n",
    "\n",
    "\n",
    "def extract_entities_relationships(folder, prompt_template):\n",
    "    start = timer()\n",
    "    files = glob.glob(f\"./data/{folder}/*\")\n",
    "    system_msg = \"You are a beauty and personal care expert who understands product relationships and routines.\"\n",
    "    print(f\"Running pipeline for {len(files)} files in {folder} folder\")\n",
    "    \n",
    "    # First pass: collect and summarize all product information\n",
    "    product_summaries = []\n",
    "    summary_prompt = \"\"\"\n",
    "    Create a concise summary of the product with key information in JSON format:\n",
    "    {\n",
    "        \"id\": \"product_id\",\n",
    "        \"name\": \"product name\",\n",
    "        \"brand\": \"brand name\",\n",
    "        \"category\": \"main category\",\n",
    "        \"price\": float,\n",
    "        \"rating\": float,\n",
    "        \"key_features\": [\"list of 3-5 most important features or benefits\"],\n",
    "        \"main_ingredients\": [\"list of key ingredients if mentioned\"],\n",
    "        \"target_concerns\": [\"list of skin/hair concerns it addresses\"],\n",
    "        \"usage_context\": \"when/how the product is meant to be used\"\n",
    "    }\n",
    "\n",
    "    Extract only the most relevant information from the description that would help identify relationships with other products.\n",
    "\n",
    "    Product text:\n",
    "    $ctext\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"First pass: Collecting product summaries...\")\n",
    "    for file in files:\n",
    "        try:\n",
    "            with open(file, \"r\", encoding='utf-8') as f:\n",
    "                text = f.read().rstrip()\n",
    "                prompt = Template(summary_prompt).substitute(ctext=text)\n",
    "                result = process_gpt(prompt, system_msg)\n",
    "                summary = json.loads(result)\n",
    "                summary['file_id'] = file.split(\"/\")[-1].replace(\".md\", \"\")\n",
    "                product_summaries.append(summary)\n",
    "                print(f\"Summarized {file.split('/')[-1]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error summarizing {file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Create a structured context string with product summaries\n",
    "    context = \"Available products in the database:\\n\"\n",
    "    for summary in product_summaries:\n",
    "        context += f\"\"\"\n",
    "        ID: {summary['file_id']}\n",
    "        Name: {summary['name']}\n",
    "        Brand: {summary['brand']}\n",
    "        Category: {summary['category']}\n",
    "        Price: {summary['price']} dollars\n",
    "        Rating: {summary['rating']}\n",
    "        Key Features: {', '.join(summary['key_features'])}\n",
    "        Main Ingredients: {', '.join(summary.get('main_ingredients', ['Not specified']))}\n",
    "        Target Concerns: {', '.join(summary.get('target_concerns', ['Not specified']))}\n",
    "        Usage: {summary['usage_context']}\n",
    "        ---\n",
    "        \"\"\"\n",
    "    \n",
    "    # Second pass: process each product with context\n",
    "    all_entities = {}\n",
    "    all_relationships = set()\n",
    "    \n",
    "    print(\"\\nSecond pass: Processing products with context...\")\n",
    "    for file in files:\n",
    "        try:\n",
    "            with open(file, \"r\", encoding='utf-8') as f:\n",
    "                text = f.read().rstrip()\n",
    "                current_product_id = file.split(\"/\")[-1].replace(\".md\", \"\")\n",
    "                \n",
    "                # Create a focused context with:\n",
    "                # 1. The full details of the current product\n",
    "                # 2. Summaries of products with similar characteristics\n",
    "                current_summary = next(s for s in product_summaries if s['file_id'] == current_product_id)\n",
    "                \n",
    "                # Filter relevant products for focused context\n",
    "                relevant_products = []\n",
    "                for summary in product_summaries:\n",
    "                    if summary['file_id'] == current_product_id:\n",
    "                        continue\n",
    "                    \n",
    "                    # Check for relevance based on various factors\n",
    "                    price_diff = abs(summary['price'] - current_summary['price'])\n",
    "                    same_brand = summary['brand'].lower() == current_summary['brand'].lower()\n",
    "                    same_category = summary['category'].lower() == current_summary['category'].lower()\n",
    "                    \n",
    "                    if (price_diff <= 10 or same_brand or same_category or \n",
    "                        any(concern in current_summary.get('target_concerns', []) \n",
    "                            for concern in summary.get('target_concerns', []))):\n",
    "                        relevant_products.append(summary)\n",
    "                \n",
    "                # Create focused context\n",
    "                focused_context = f\"\"\"\n",
    "                Current Product Full Details:\n",
    "                {text}\n",
    "\n",
    "                Relevant Products for Relationship Consideration:\n",
    "                \"\"\"\n",
    "                \n",
    "                for prod in relevant_products[:10]:  # Limit to 10 most relevant products\n",
    "                    focused_context += f\"\"\"\n",
    "                    ID: {prod['file_id']}\n",
    "                    Name: {prod['name']}\n",
    "                    Brand: {prod['brand']}\n",
    "                    Category: {prod['category']}\n",
    "                    Price: {prod['price']} dollars\n",
    "                    Rating: {prod['rating']}\n",
    "                    Key Features: {', '.join(prod['key_features'])}\n",
    "                    Main Ingredients: {', '.join(prod.get('main_ingredients', ['Not specified']))}\n",
    "                    Target Concerns: {', '.join(prod.get('target_concerns', ['Not specified']))}\n",
    "                    Usage: {prod['usage_context']}\n",
    "                    ---\n",
    "                    \"\"\"\n",
    "                \n",
    "                # Add focused context to the prompt\n",
    "                full_prompt = f\"\"\"\n",
    "                {prompt_template}\n",
    "\n",
    "                {focused_context}\n",
    "\n",
    "                Important: Consider relationships with other available products based on:\n",
    "                1. Similar price ranges (within 10 dollars)\n",
    "                2. Same brand or manufacturer\n",
    "                3. Complementary products in beauty routines\n",
    "                4. Similar product categories or subcategories\n",
    "                5. Common target concerns or purposes\n",
    "                6. Compatible ingredients\n",
    "                7. Similar usage contexts\n",
    "                8. Routine sequence (products used before/after)\n",
    "                \"\"\"\n",
    "                \n",
    "                result = process_gpt(full_prompt, system_msg)\n",
    "                parsed_result = json.loads(result)\n",
    "                \n",
    "                # Add entities and relationships\n",
    "                for entity in parsed_result[\"entities\"]:\n",
    "                    entity_id = entity[\"id\"]\n",
    "                    if entity_id in all_entities:\n",
    "                        all_entities[entity_id].update(entity)\n",
    "                    else:\n",
    "                        all_entities[entity_id] = entity\n",
    "                \n",
    "                all_relationships.update(parsed_result[\"relationships\"])\n",
    "                print(f\"Processed {file.split('/')[-1]}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    final_result = {\n",
    "        \"entities\": list(all_entities.values()),\n",
    "        \"relationships\": list(all_relationships)\n",
    "    }\n",
    "    \n",
    "    end = timer()\n",
    "    print(f\"Pipeline completed in {end-start} seconds\")\n",
    "    return [final_result]\n",
    "\n",
    "\n",
    "# def clean_text(text):\n",
    "#     \"\"\"Clean text to prevent JSON parsing issues\"\"\"\n",
    "#     # Replace problematic characters\n",
    "#     text = text.replace('\"', \"'\")  # Replace double quotes with single quotes\n",
    "#     text = text.replace('\\\\', '/')  # Replace backslashes with forward slashes\n",
    "#     text = text.encode('ascii', 'ignore').decode()  # Remove non-ASCII characters\n",
    "#     return text\n",
    "\n",
    "# def extract_entities_relationships(folder, prompt_template):\n",
    "#     start = timer()\n",
    "#     files = glob.glob(f\"./data/{folder}/*\")\n",
    "#     system_msg = \"You are a beauty and personal care expert who understands product relationships and routines.\"\n",
    "#     print(f\"Running pipeline for {len(files)} files in {folder} folder\")\n",
    "    \n",
    "#     # First pass: collect and summarize all product information\n",
    "#     product_summaries = {}  # Use dict with file_id as key for better tracking\n",
    "#     summary_prompt = \"\"\"\n",
    "#     Create a concise summary of the product. Return ONLY valid JSON without any explanatory text:\n",
    "#     {\n",
    "#         \"product_id\": \"KEEP_ORIGINAL_ID\",  # Do not modify the original product ID\n",
    "#         \"name\": \"product name\",\n",
    "#         \"brand\": \"brand name\",\n",
    "#         \"category\": \"main category\",\n",
    "#         \"price\": float,\n",
    "#         \"rating\": float,\n",
    "#         \"key_features\": [\"feature1\", \"feature2\", \"feature3\"],\n",
    "#         \"main_ingredients\": [\"ingredient1\", \"ingredient2\"],\n",
    "#         \"target_concerns\": [\"concern1\", \"concern2\"],\n",
    "#         \"usage\": \"brief usage description\"\n",
    "#     }\n",
    "\n",
    "#     Note: Maintain the exact product ID from the input file.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     print(\"First pass: Collecting product summaries...\")\n",
    "#     for file in files:\n",
    "#         try:\n",
    "#             file_id = file.split(\"/\")[-1].replace(\".md\", \"\")\n",
    "#             with open(file, \"r\", encoding='utf-8') as f:\n",
    "#                 text = clean_text(f.read().rstrip())\n",
    "#                 prompt = Template(summary_prompt).substitute(ctext=text)\n",
    "#                 result = process_gpt(prompt, system_msg)\n",
    "#                 summary = json.loads(result)\n",
    "#                 summary['file_id'] = file_id  # Ensure we keep the original file ID\n",
    "#                 product_summaries[file_id] = summary\n",
    "#                 print(f\"Summarized {file_id}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error summarizing {file}: {e}\")\n",
    "#             continue\n",
    "    \n",
    "#     # Create a structured context string with product summaries\n",
    "#     def create_focused_context(current_id, all_summaries, max_related=10):\n",
    "#         \"\"\"Create context focusing on relevant products\"\"\"\n",
    "#         current_product = all_summaries[current_id]\n",
    "#         other_products = []\n",
    "        \n",
    "#         for pid, summary in all_summaries.items():\n",
    "#             if pid == current_id:\n",
    "#                 continue\n",
    "                \n",
    "#             # Calculate relevance score\n",
    "#             relevance = 0\n",
    "#             if abs(summary['price'] - current_product['price']) <= 10:\n",
    "#                 relevance += 1\n",
    "#             if summary['brand'].lower() == current_product['brand'].lower():\n",
    "#                 relevance += 2\n",
    "#             if summary['category'].lower() == current_product['category'].lower():\n",
    "#                 relevance += 2\n",
    "            \n",
    "#             other_products.append((relevance, summary))\n",
    "        \n",
    "#         # Sort by relevance and take top N\n",
    "#         other_products.sort(reverse=True, key=lambda x: x[0])\n",
    "#         related_products = other_products[:max_related]\n",
    "        \n",
    "#         # Create context string\n",
    "#         context = \"Related products for consideration:\\n\"\n",
    "#         for _, prod in related_products:\n",
    "#             context += f\"\"\"\n",
    "# ID: {prod['file_id']}  # Original product ID\n",
    "# Name: {clean_text(prod['name'])}\n",
    "# Brand: {clean_text(prod['brand'])}\n",
    "# Category: {clean_text(prod['category'])}\n",
    "# Price: ${prod['price']}\n",
    "# Rating: {prod['rating']}\n",
    "# Key Features: {', '.join(clean_text(f) for f in prod['key_features'])}\n",
    "# Usage: {clean_text(prod['usage'])}\n",
    "# ---\"\"\"\n",
    "        \n",
    "#         return context\n",
    "    \n",
    "#     # Second pass: process each product with context\n",
    "#     all_entities = {}\n",
    "#     all_relationships = set()\n",
    "    \n",
    "#     print(\"\\nSecond pass: Processing products with context...\")\n",
    "#     for file in files:\n",
    "#         try:\n",
    "#             file_id = file.split(\"/\")[-1].replace(\".md\", \"\")\n",
    "#             with open(file, \"r\", encoding='utf-8') as f:\n",
    "#                 text = clean_text(f.read().rstrip())\n",
    "                \n",
    "#                 # Create focused context\n",
    "#                 focused_context = create_focused_context(file_id, product_summaries)\n",
    "                \n",
    "#                 # Modify prompt to ensure consistent product IDs\n",
    "#                 relationship_prompt = f\"\"\"\n",
    "#                 Analyze the current product and create relationships with other products.\n",
    "#                 IMPORTANT: \n",
    "#                 - Use EXACT product IDs as provided in the context\n",
    "#                 - Do not create new product IDs\n",
    "#                 - Do not modify existing product IDs\n",
    "#                 - Ensure all relationships reference valid product IDs from the context\n",
    "\n",
    "#                 Current Product ID: {file_id}\n",
    "#                 {focused_context}\n",
    "#                 \"\"\"\n",
    "                \n",
    "#                 full_prompt = f\"{prompt_template}\\n\\n{relationship_prompt}\\n\\nProduct to analyze:\\n{text}\"\n",
    "                \n",
    "#                 result = process_gpt(full_prompt, system_msg)\n",
    "#                 parsed_result = json.loads(result)\n",
    "                \n",
    "#                 # Validate and add entities\n",
    "#                 for entity in parsed_result[\"entities\"]:\n",
    "#                     entity_id = entity[\"id\"]\n",
    "#                     if entity_id in all_entities:\n",
    "#                         # Update only if it's the same product\n",
    "#                         if entity.get(\"label\") == \"Product\" and entity_id != file_id:\n",
    "#                             continue  # Skip if it's trying to redefine another product\n",
    "#                         all_entities[entity_id].update(entity)\n",
    "#                     else:\n",
    "#                         all_entities[entity_id] = entity\n",
    "                \n",
    "#                 # Validate relationships\n",
    "#                 for rel in parsed_result[\"relationships\"]:\n",
    "#                     src_id, rel_type, tgt_id = rel.split(\"|\")\n",
    "#                     # Only add relationship if both products exist in our context\n",
    "#                     if src_id in product_summaries and tgt_id in product_summaries:\n",
    "#                         all_relationships.add(rel)\n",
    "                \n",
    "#                 print(f\"Processed {file_id}\")\n",
    "                \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing {file}: {e}\")\n",
    "#             continue\n",
    "    \n",
    "#     final_result = {\n",
    "#         \"entities\": list(all_entities.values()),\n",
    "#         \"relationships\": list(all_relationships)\n",
    "#     }\n",
    "    \n",
    "#     end = timer()\n",
    "#     print(f\"Pipeline completed in {end-start} seconds\")\n",
    "#     return [final_result]\n",
    "\n",
    "# # Function to take folder of files and a prompt template, and return a json-object of all the entities and relationships\n",
    "# def extract_entities_relationships(folder, prompt_template):\n",
    "#     start = timer()\n",
    "#     files = glob.glob(f\"./data/{folder}/*\")\n",
    "#     system_msg = \"You are a helpful IT-project and account management expert who extracts information from documents.\"\n",
    "#     print(f\"Running pipeline for {len(files)} files in {folder} folder\")\n",
    "    \n",
    "#     # Dictionary to store all entities and relationships\n",
    "#     all_entities = {}  # Use ID as key to avoid duplicates\n",
    "#     all_relationships = set()  # Use set to avoid duplicates\n",
    "    \n",
    "#     for i, file in enumerate(files):\n",
    "#         print(f\"Extracting entities and relationships for {file}\")\n",
    "#         try:\n",
    "#             with open(file, \"r\", encoding='utf-8') as f:\n",
    "#                 text = f.read().rstrip()\n",
    "#                 prompt = Template(prompt_template).substitute(ctext=text)\n",
    "#                 result = process_gpt(prompt, system_msg=system_msg)\n",
    "#                 parsed_result = json.loads(result)\n",
    "                \n",
    "#                 # Add entities to master dictionary, using ID as key to avoid duplicates\n",
    "#                 for entity in parsed_result[\"entities\"]:\n",
    "#                     entity_id = entity[\"id\"]\n",
    "#                     if entity_id in all_entities:\n",
    "#                         # Merge attributes if entity already exists\n",
    "#                         all_entities[entity_id].update(entity)\n",
    "#                     else:\n",
    "#                         all_entities[entity_id] = entity\n",
    "                \n",
    "#                 # Add relationships to master set\n",
    "#                 all_relationships.update(parsed_result[\"relationships\"])\n",
    "                \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing {file}: {e}\")\n",
    "    \n",
    "#     # Convert back to the expected format\n",
    "#     final_result = {\n",
    "#         \"entities\": list(all_entities.values()),\n",
    "#         \"relationships\": list(all_relationships)\n",
    "#     }\n",
    "    \n",
    "#     end = timer()\n",
    "#     print(f\"Pipeline completed in {end-start} seconds\")\n",
    "#     return [final_result]\n",
    "\n",
    "# Update the ingestion_pipeline function to handle the merged results\n",
    "def ingestion_pipeline(folders):\n",
    "    entities_relationships = []\n",
    "    for key, value in folders.items():\n",
    "        entities_relationships.extend(extract_entities_relationships(key, value))\n",
    "    \n",
    "    # Generate and execute cypher statements\n",
    "    cypher_statements = generate_cypher(entities_relationships)\n",
    "    for i, stmt in enumerate(cypher_statements):\n",
    "        print(f\"Executing cypher statement {i+1} of {len(cypher_statements)}\\\"\")\n",
    "        try:\n",
    "            gds.execute_query(stmt)\n",
    "        except Exception as e:\n",
    "            with open(\"failed_statements.txt\", \"w\") as f:\n",
    "                f.write(f\"{stmt} - Exception: {e}\\n\")\n",
    "\n",
    "\n",
    "# Function to take a json-object of entitites and relationships and generate cypher query for creating those entities\n",
    "# def generate_cypher(json_obj):\n",
    "#     e_statements = []\n",
    "#     r_statements = []\n",
    "\n",
    "#     e_label_map = {}\n",
    "\n",
    "#     # loop through our json object\n",
    "#     for i, obj in enumerate(json_obj):\n",
    "#         print(f\"Generating cypher for file {i+1} of {len(json_obj)}\")\n",
    "#         for entity in obj[\"entities\"]:\n",
    "#             label = entity[\"label\"]\n",
    "#             id = entity[\"id\"]\n",
    "#             id = id.replace(\"-\", \"\").replace(\"_\", \"\")\n",
    "#             properties = {k: v for k, v in entity.items() if k not in [\"label\", \"id\"]}\n",
    "\n",
    "#             cypher = f'MERGE (n:{label} {{id: \"{id}\"}})'\n",
    "#             if properties:\n",
    "#                 props_str = \", \".join(\n",
    "#                     [f'n.{key} = \"{val}\"' for key, val in properties.items()]\n",
    "#                 )\n",
    "#                 cypher += f\" ON CREATE SET {props_str}\"\n",
    "#             e_statements.append(cypher)\n",
    "#             e_label_map[id] = label\n",
    "\n",
    "#         for rs in obj[\"relationships\"]:\n",
    "#             src_id, rs_type, tgt_id = rs.split(\"|\")\n",
    "#             src_id = src_id.replace(\"-\", \"\").replace(\"_\", \"\")\n",
    "#             tgt_id = tgt_id.replace(\"-\", \"\").replace(\"_\", \"\")\n",
    "\n",
    "#             src_label = e_label_map[src_id]\n",
    "#             tgt_label = e_label_map[tgt_id]\n",
    "\n",
    "#             cypher = f'MERGE (a:{src_label} {{id: \"{src_id}\"}}) MERGE (b:{tgt_label} {{id: \"{tgt_id}\"}}) MERGE (a)-[:{rs_type}]->(b)'\n",
    "#             r_statements.append(cypher)\n",
    "\n",
    "#     with open(\"cyphers.txt\", \"w\") as outfile:\n",
    "#         outfile.write(\"\\n\".join(e_statements + r_statements))\n",
    "\n",
    "#     return e_statements + r_statements\n",
    "def generate_cypher(json_obj):\n",
    "    e_statements = []\n",
    "    r_statements = []\n",
    "    e_label_map = {}\n",
    "    skipped_relationships = []\n",
    "\n",
    "    # First pass: collect all entities\n",
    "    for i, obj in enumerate(json_obj):\n",
    "        print(f\"Generating cypher for file {i+1} of {len(json_obj)}\")\n",
    "        for entity in obj[\"entities\"]:\n",
    "            label = entity[\"label\"]\n",
    "            id = entity[\"id\"]\n",
    "            id = id.replace(\"-\", \"\").replace(\"_\", \"\")\n",
    "            properties = {k: v for k, v in entity.items() if k not in [\"label\", \"id\"]}\n",
    "\n",
    "            cypher = f'MERGE (n:{label} {{id: \"{id}\"}})'\n",
    "            if properties:\n",
    "                props_str = \", \".join(\n",
    "                    [f'n.{key} = \"{val}\"' for key, val in properties.items()]\n",
    "                )\n",
    "                cypher += f\" ON CREATE SET {props_str}\"\n",
    "            e_statements.append(cypher)\n",
    "            e_label_map[id] = label\n",
    "\n",
    "    # Second pass: create relationships only between existing entities\n",
    "    for i, obj in enumerate(json_obj):\n",
    "        for rs in obj[\"relationships\"]:\n",
    "            try:\n",
    "                src_id, rs_type, tgt_id = rs.split(\"|\")\n",
    "                src_id = src_id.replace(\"-\", \"\").replace(\"_\", \"\")\n",
    "                tgt_id = tgt_id.replace(\"-\", \"\").replace(\"_\", \"\")\n",
    "\n",
    "                # Check if both source and target entities exist\n",
    "                if src_id in e_label_map and tgt_id in e_label_map:\n",
    "                    src_label = e_label_map[src_id]\n",
    "                    tgt_label = e_label_map[tgt_id]\n",
    "\n",
    "                    cypher = f'MERGE (a:{src_label} {{id: \"{src_id}\"}}) MERGE (b:{tgt_label} {{id: \"{tgt_id}\"}}) MERGE (a)-[:{rs_type}]->(b)'\n",
    "                    r_statements.append(cypher)\n",
    "                else:\n",
    "                    skipped_relationships.append(rs)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing relationship {rs}: {e}\")\n",
    "                skipped_relationships.append(rs)\n",
    "\n",
    "    # Log skipped relationships\n",
    "    if skipped_relationships:\n",
    "        print(\"\\nWarning: The following relationships were skipped due to missing entities:\")\n",
    "        for rs in skipped_relationships:\n",
    "            print(f\"- {rs}\")\n",
    "        \n",
    "        with open(\"skipped_relationships.txt\", \"w\") as f:\n",
    "            f.write(\"\\n\".join(skipped_relationships))\n",
    "            print(\"\\nSkipped relationships have been written to 'skipped_relationships.txt'\")\n",
    "\n",
    "    # Write all valid statements to file\n",
    "    with open(\"cyphers.txt\", \"w\") as outfile:\n",
    "        outfile.write(\"\\n\".join(e_statements + r_statements))\n",
    "\n",
    "    return e_statements + r_statements\n",
    "\n",
    "\n",
    "# Final function to bring all the steps together\n",
    "def ingestion_pipeline(folders):\n",
    "    # Extrating the entites and relationships from each folder, append into one json_object\n",
    "    entities_relationships = []\n",
    "    for key, value in folders.items():\n",
    "        entities_relationships.extend(extract_entities_relationships(key, value))\n",
    "\n",
    "    # Generate and execute cypher statements\n",
    "    cypher_statements = generate_cypher(entities_relationships)\n",
    "    for i, stmt in enumerate(cypher_statements):\n",
    "        print(f\"Executing cypher statement {i+1} of {len(cypher_statements)}\")\n",
    "        try:\n",
    "            gds.execute_query(stmt)\n",
    "        except Exception as e:\n",
    "            with open(\"failed_statements.txt\", \"w\") as f:\n",
    "                f.write(f\"{stmt} - Exception: {e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Defining Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_products_prompt_template = \"\"\"\n",
    "From the product description below, extract Entities & Relationships in an open format. You are free to:\n",
    "1. Create any entity types you identify, with any relevant attributes. Each entity must have at least:\n",
    "    - label: The type of entity (e.g., 'Product', 'Ingredient', 'Benefit', etc.)\n",
    "    - id: A unique alphanumeric identifier\n",
    "    - Additional attributes specific to that entity type\n",
    "\n",
    "2. Create any relationships you can identify:\n",
    "    - Between entities within this product description\n",
    "        - SAME_BRAND_AS: Products from the same manufacturer\n",
    "        - SIMILAR_PRICE_TIER_AS: Products within 5 dollar price range\n",
    "        - SIMILAR_RATING_AS: Products with similar ratings (within 0.5 rating score)\n",
    "        - USED_WITH: Products commonly used together in a routine\n",
    "        - ALTERNATIVE_TO: Similar products that serve the same purpose\n",
    "        - COMPLEMENTS: Products that enhance each other's effects\n",
    "        - PART_OF_COLLECTION: Products in the same product line\n",
    "        - SHARES_INGREDIENT_WITH: Products with common key ingredients\n",
    "        - ADDRESSES_SAME_CONCERN: Products targeting the same issues\n",
    "        - USED_IN_SAME_STEP: Products used at the same routine step\n",
    "    - With potential related products (use consistent IDs for products you reference)\n",
    "    - With any beauty/health concepts, ingredients, or categories\n",
    "\n",
    "3. The output MUST be valid JSON and should be formatted as:\n",
    "{\n",
    "    \"entities\": [\n",
    "        {\"label\":\"string\",\"id\":\"string\",...additional_attributes},\n",
    "        ...\n",
    "    ],\n",
    "    \"relationships\": [\n",
    "        \"sourceid|RELATIONSHIP_TYPE|targetid\",\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "\n",
    "Important: When referencing other products or creating relationships across products:\n",
    "- Use predictable, consistent IDs for well-known products or concepts\n",
    "- If you reference another product, include it as an entity with as much information as you can infer\n",
    "- Create meaningful relationship types that explain how products connect\n",
    "\n",
    "Case Sheet:\n",
    "$ctext\n",
    "\"\"\"\n",
    "\n",
    "# amazon_products_prompt_template = \"\"\"\n",
    "# From the product description below, extract Entities & Relationships in an open format, ensuring connections to other beauty/personal care products. Consider:\n",
    "\n",
    "# 1. Entity Types (include but not limited to):\n",
    "#     - Products (with price, rating, brand, type)\n",
    "#     - Brands/Manufacturers\n",
    "#     - Product Categories and Subcategories\n",
    "#     - Price Ranges (0 to 15 dollars: Budget, 15 to 30 dollars: Mid-range, 30 or more dollars: Premium)\n",
    "#     - Rating Tiers (4.5 to 5: Excellent, 4 to 4.4: Good, 3 to 3.9: Average, less than 3: Poor)\n",
    "#     - Usage Occasions (Morning, Night, Special Event, etc.)\n",
    "#     - Target Concerns (Aging, Acne, Dryness, etc.)\n",
    "#     - Key Ingredients\n",
    "#     - Product Functions (Cleansing, Moisturizing, Protection, etc.)\n",
    "\n",
    "# 2. Cross-Product Relationships:\n",
    "#     - SAME_BRAND_AS: Products from the same manufacturer\n",
    "#     - SIMILAR_PRICE_TIER_AS: Products in the same price range\n",
    "#     - SIMILAR_RATING_AS: Products with similar ratings (within 0.5 points)\n",
    "#     - USED_WITH: Products commonly used together in a routine\n",
    "#     - ALTERNATIVE_TO: Similar products that serve the same purpose\n",
    "#     - COMPLEMENTS: Products that enhance each other's effects\n",
    "#     - PART_OF_COLLECTION: Products in the same product line\n",
    "#     - SHARES_INGREDIENT_WITH: Products with common key ingredients\n",
    "#     - ADDRESSES_SAME_CONCERN: Products targeting the same issues\n",
    "#     - USED_IN_SAME_STEP: Products used at the same routine step\n",
    "\n",
    "# 3. The output MUST be valid JSON and should be formatted as:\n",
    "# {\n",
    "#     \"entities\": [\n",
    "#         {\n",
    "#             \"label\": \"string\",\n",
    "#             \"id\": \"string\",\n",
    "#             \"name\": \"string\",\n",
    "#             \"price\": float,\n",
    "#             \"rating\": float,\n",
    "#             \"brand\": \"string\",\n",
    "#             \"category\": \"string\",\n",
    "#             ... (additional attributes as needed)\n",
    "#         }\n",
    "#     ],\n",
    "#     \"relationships\": [\n",
    "#         \"sourceid|RELATIONSHIP_TYPE|targetid\"\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# Important Notes:\n",
    "# - Create relationships with other likely related products even if not explicitly mentioned\n",
    "# - Use consistent IDs for brands, categories, and common product types\n",
    "# - Consider price points and ratings when suggesting relationships\n",
    "# - Think about typical beauty/personal care routines when creating USED_WITH relationships\n",
    "# - Include relationships to generic product categories (e.g., \"moisturizer\", \"cleanser\")\n",
    "# - Consider seasonal or occasion-based relationships\n",
    "# - Look for complementary products in typical beauty routines\n",
    "\n",
    "# Case Sheet:\n",
    "# $ctext\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Running the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running pipeline for 5 files in amazon_products folder\n",
      "First pass: Collecting product summaries...\n",
      "Summarized unstructured2\n",
      "Summarized unstructured3\n",
      "Summarized unstructured4\n",
      "Summarized unstructured5\n",
      "Summarized unstructured1\n",
      "\n",
      "Second pass: Processing products with context...\n",
      "Processed unstructured2\n",
      "Processed unstructured3\n",
      "Processed unstructured4\n",
      "Processed unstructured5\n",
      "Processed unstructured1\n",
      "Pipeline completed in 223.90520516596735 seconds\n",
      "Generating cypher for file 1 of 1\n",
      "Executing cypher statement 1 of 5\n",
      "Executing cypher statement 2 of 5\n",
      "Executing cypher statement 3 of 5\n",
      "Executing cypher statement 4 of 5\n",
      "Executing cypher statement 5 of 5\n"
     ]
    }
   ],
   "source": [
    "folders = {\n",
    "    \"amazon_products\": amazon_products_prompt_template,\n",
    "}\n",
    "ingestion_pipeline(folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_cypher_from_file(filepath):\n",
    "    \"\"\"\n",
    "    Reads Cypher queries from a text file and executes them sequentially.\n",
    "    \n",
    "    Args:\n",
    "        filepath (str): Path to the text file containing Cypher queries (one per line)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the cypher statements from file\n",
    "        with open(filepath, 'r') as file:\n",
    "            cypher_statements = [line.strip() for line in file if line.strip()]\n",
    "        \n",
    "        print(f\"Found {len(cypher_statements)} Cypher statements to execute\")\n",
    "        \n",
    "        # Execute each statement\n",
    "        failed_statements = []\n",
    "        for i, stmt in enumerate(cypher_statements, 1):\n",
    "            try:\n",
    "                print(f\"Executing statement {i} of {len(cypher_statements)}\")\n",
    "                gds.execute_query(stmt)\n",
    "            except Exception as e:\n",
    "                print(f\"Error executing statement {i}: {e}\")\n",
    "                failed_statements.append((stmt, str(e)))\n",
    "        \n",
    "        # Report results\n",
    "        success_count = len(cypher_statements) - len(failed_statements)\n",
    "        print(f\"\\nExecution complete:\")\n",
    "        print(f\"Successfully executed: {success_count}/{len(cypher_statements)} statements\")\n",
    "        \n",
    "        # If there were any failures, write them to a file\n",
    "        if failed_statements:\n",
    "            print(f\"Failed statements: {len(failed_statements)}\")\n",
    "            with open(\"failed_statements.txt\", \"w\") as f:\n",
    "                for stmt, error in failed_statements:\n",
    "                    f.write(f\"Statement: {stmt}\\nError: {error}\\n\\n\")\n",
    "            print(\"Failed statements have been written to 'failed_statements.txt'\")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Could not find file '{filepath}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "    finally:\n",
    "        # Ensure the database connection is closed properly\n",
    "        gds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 64 Cypher statements to execute\n",
      "Executing statement 1 of 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cf/2y7jfcdj6dbfw7ljc4s4jpv40000gn/T/ipykernel_36087/4009459450.py:20: DeprecationWarning: Using a driver after it has been closed is deprecated. Future versions of the driver will raise an error.\n",
      "  gds.execute_query(stmt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing statement 2 of 64\n",
      "Executing statement 3 of 64\n",
      "Executing statement 4 of 64\n",
      "Executing statement 5 of 64\n",
      "Executing statement 6 of 64\n",
      "Executing statement 7 of 64\n",
      "Executing statement 8 of 64\n",
      "Executing statement 9 of 64\n",
      "Executing statement 10 of 64\n",
      "Executing statement 11 of 64\n",
      "Executing statement 12 of 64\n",
      "Executing statement 13 of 64\n",
      "Executing statement 14 of 64\n",
      "Executing statement 15 of 64\n",
      "Executing statement 16 of 64\n",
      "Executing statement 17 of 64\n",
      "Executing statement 18 of 64\n",
      "Executing statement 19 of 64\n",
      "Executing statement 20 of 64\n",
      "Executing statement 21 of 64\n",
      "Executing statement 22 of 64\n",
      "Executing statement 23 of 64\n",
      "Executing statement 24 of 64\n",
      "Executing statement 25 of 64\n",
      "Executing statement 26 of 64\n",
      "Executing statement 27 of 64\n",
      "Executing statement 28 of 64\n",
      "Executing statement 29 of 64\n",
      "Executing statement 30 of 64\n",
      "Executing statement 31 of 64\n",
      "Executing statement 32 of 64\n",
      "Executing statement 33 of 64\n",
      "Executing statement 34 of 64\n",
      "Executing statement 35 of 64\n",
      "Executing statement 36 of 64\n",
      "Executing statement 37 of 64\n",
      "Executing statement 38 of 64\n",
      "Executing statement 39 of 64\n",
      "Executing statement 40 of 64\n",
      "Executing statement 41 of 64\n",
      "Executing statement 42 of 64\n",
      "Executing statement 43 of 64\n",
      "Executing statement 44 of 64\n",
      "Executing statement 45 of 64\n",
      "Executing statement 46 of 64\n",
      "Executing statement 47 of 64\n",
      "Executing statement 48 of 64\n",
      "Executing statement 49 of 64\n",
      "Executing statement 50 of 64\n",
      "Executing statement 51 of 64\n",
      "Executing statement 52 of 64\n",
      "Executing statement 53 of 64\n",
      "Executing statement 54 of 64\n",
      "Executing statement 55 of 64\n",
      "Executing statement 56 of 64\n",
      "Executing statement 57 of 64\n",
      "Executing statement 58 of 64\n",
      "Executing statement 59 of 64\n",
      "Executing statement 60 of 64\n",
      "Executing statement 61 of 64\n",
      "Executing statement 62 of 64\n",
      "Executing statement 63 of 64\n",
      "Executing statement 64 of 64\n",
      "\n",
      "Execution complete:\n",
      "Successfully executed: 64/64 statements\n"
     ]
    }
   ],
   "source": [
    "execute_cypher_from_file(\"cyphers.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Capstone311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
