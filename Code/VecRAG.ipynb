{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector RAG Pipeline\n",
    "1. QA pipeline with LangChain\n",
    "2. Persistent vector storage based on ChromaDB\n",
    "3. Evaluation with ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import huggingface_hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('private-config.env'):\n",
    "    load_dotenv('private-config.env', override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "hf_token = os.getenv('HF_TOKEN')\n",
    "# Use the token with the Hugging Face library\n",
    "huggingface_hub.login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "# model_kwargs = {\"device\": \"cpu\"}\n",
    "# encode_kwargs = {\"normalize_embeddings\": False}\n",
    "# embeddings = HuggingFaceEmbeddings(\n",
    "#     model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 13, 'total_tokens': 43, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-3e9b18a6-29ff-407b-b1d3-bf6138383198-0', usage_metadata={'input_tokens': 13, 'output_tokens': 30, 'total_tokens': 43, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test LLM\n",
    "response = llm.invoke(\"Hello, how are you?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_docs = [\n",
    "    \"Albert Einstein proposed the theory of relativity, which transformed our understanding of time, space, and gravity.\",\n",
    "    \"Marie Curie was a physicist and chemist who conducted pioneering research on radioactivity and won two Nobel Prizes.\",\n",
    "    \"Isaac Newton formulated the laws of motion and universal gravitation, laying the foundation for classical mechanics.\",\n",
    "    \"Charles Darwin introduced the theory of evolution by natural selection in his book 'On the Origin of Species'.\",\n",
    "    \"Ada Lovelace is regarded as the first computer programmer for her work on Charles Babbage's early mechanical computer, the Analytical Engine.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pq/pzj9g_pd2vl6fsmjt6t01msw0000gn/T/ipykernel_97516/133973202.py:7: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  db.persist()\n"
     ]
    }
   ],
   "source": [
    "documents = [Document(page_content=doc) for doc in sample_docs]\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "docstore = text_splitter.split_documents(documents)\n",
    "\n",
    "# Create ChromaDB and store embeddings\n",
    "db = Chroma.from_documents(docstore, embeddings, persist_directory=\"./chroma_db\")\n",
    "\n",
    "# Create a retriever\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Context:\n",
      " 'On the Origin of Species'.\n",
      "\n",
      "'On the Origin of Species'.\n",
      "\n",
      "'On the Origin of Species'.\n",
      "\n",
      "the foundation for classical mechanics.\n",
      "\n",
      "the foundation for classical mechanics.\n"
     ]
    }
   ],
   "source": [
    "# Querying ChromaDB to Get Context\n",
    "query = \"What is the key takeaway from the document?\"\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "retrieved_context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "print(\"Retrieved Context:\\n\", retrieved_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Response:\n",
      " The key takeaway from \"On the Origin of Species\" is the theory of evolution by natural selection, which explains how species adapt and evolve over time through the process of variation and survival of the fittest.\n"
     ]
    }
   ],
   "source": [
    "# Define an LLM and Generate Response Using Context\n",
    "prompt = f\"\"\"\n",
    "Based on the following context, answer the question:\\n\n",
    "Context: \\n\n",
    "{retrieved_context}\\n\n",
    "Question: {query}\\n\n",
    "Answer:\n",
    "\"\"\"\n",
    "response = llm.invoke(prompt)\n",
    "print(\"\\nGenerated Response:\\n\", response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_queries = [\n",
    "    \"Who introduced the theory of relativity?\",\n",
    "    \"Who was the first computer programmer?\",\n",
    "    \"What did Isaac Newton contribute to science?\",\n",
    "    \"Who won two Nobel Prizes for research on radioactivity?\",\n",
    "    \"What is the theory of evolution by natural selection?\"\n",
    "]\n",
    "\n",
    "expected_responses = [\n",
    "    \"Albert Einstein proposed the theory of relativity, which transformed our understanding of time, space, and gravity.\",\n",
    "    \"Ada Lovelace is regarded as the first computer programmer for her work on Charles Babbage's early mechanical computer, the Analytical Engine.\",\n",
    "    \"Isaac Newton formulated the laws of motion and universal gravitation, laying the foundation for classical mechanics.\",\n",
    "    \"Marie Curie was a physicist and chemist who conducted pioneering research on radioactivity and won two Nobel Prizes.\",\n",
    "    \"Charles Darwin introduced the theory of evolution by natural selection in his book 'On the Origin of Species'.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:05,  1.04s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "dataset = []\n",
    "\n",
    "for query,reference in tqdm(zip(sample_queries,expected_responses)):\n",
    "\n",
    "    relevant_docs = retriever.invoke(query)\n",
    "    relevant_context = \"\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "    prompt = f\"Based on the following context, answer the question:\\n\\n{relevant_context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    response = llm.invoke(prompt).content\n",
    "    dataset.append(\n",
    "        {\n",
    "            \"user_input\":query,\n",
    "            \"retrieved_contexts\":[d.page_content for d in relevant_docs],\n",
    "            \"response\":response,\n",
    "            \"reference\":reference\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import EvaluationDataset\n",
    "evaluation_dataset = EvaluationDataset.from_list(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a818bb17794d6199145258c56d05e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'context_recall': 1.0000, 'faithfulness': 0.9000, 'factual_correctness': 0.7800, 'answer_correctness': 0.6608, 'answer_relevancy': 0.9554, 'context_precision': 0.7717}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, AnswerCorrectness, AnswerRelevancy, ContextRecall, ContextPrecision\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "metrics_list = [\n",
    "    LLMContextRecall(),\n",
    "    Faithfulness(),\n",
    "    FactualCorrectness(),\n",
    "    AnswerCorrectness(),\n",
    "    AnswerRelevancy(),\n",
    "    ContextRecall(),\n",
    "    ContextPrecision()\n",
    "]\n",
    "result = evaluate(dataset=evaluation_dataset,metrics=metrics_list,llm=evaluator_llm)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02069312 1.1799001  2.79940678] [[-0.72394434  0.07764021 -0.68547545]\n",
      " [ 0.63643914  0.45855781 -0.62021767]\n",
      " [-0.26617629  0.88526647  0.3813836 ]]\n",
      "[[-0.6855  0.0776]\n",
      " [-0.6202  0.4586]\n",
      " [ 0.3814  0.8853]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "def pca(data: np.ndarray, k: int) -> np.ndarray:\n",
    "    n, d = data.shape\n",
    "    data = (data - data.mean(axis=0)) / np.std(data, axis=0)\n",
    "    cov = data.T @ data / (n-1)\n",
    "    eigval, eigvec = np.linalg.eigh(cov)\n",
    "    idx = np.argsort(eigval)[::-1]\n",
    "    print(eigval, eigvec)\n",
    "    principal_components = eigvec[:, idx[:k]]\n",
    "    return np.round(principal_components, 4)\n",
    "\n",
    "print(pca(np.array([[4,2,1],[5,6,7],[9,12,1],[4,6,7]]),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.79940678 0.02069312 1.1799001 ] [[ 0.68547545  0.72394434  0.07764021]\n",
      " [ 0.62021767 -0.63643914  0.45855781]\n",
      " [-0.3813836   0.26617629  0.88526647]]\n",
      "[[ 0.6855  0.0776]\n",
      " [ 0.6202  0.4586]\n",
      " [-0.3814  0.8853]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def pca(data, k):\n",
    "    # Standardize the data\n",
    "    data_standardized = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n",
    "    \n",
    "    # Compute the covariance matrix\n",
    "    covariance_matrix = np.cov(data_standardized, rowvar=False)\n",
    "    \n",
    "    # Eigen decomposition\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n",
    "    print(eigenvalues, eigenvectors)\n",
    "    \n",
    "    # Sort the eigenvectors by decreasing eigenvalues\n",
    "    idx = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues_sorted = eigenvalues[idx]\n",
    "    eigenvectors_sorted = eigenvectors[:,idx]\n",
    "    \n",
    "    # Select the top k eigenvectors (principal components)\n",
    "    principal_components = eigenvectors_sorted[:, :k]\n",
    "    \n",
    "    return np.round(principal_components, 4)\n",
    "\n",
    "data = np.array([[4,2,1],[5,6,7],[9,12,1],[4,6,7]])\n",
    "\n",
    "print(pca(data, 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
